<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>CMU-MOSEI Sentiment | Jugal Gajjar</title>
  <link rel="stylesheet" href="../styles-projects.css" />
  <script src="../script.js" defer></script>
</head>

<body>
  <header>
    <h1>Multimodal Sentiment Analysis</h1>
    <p>Transformer-based sentiment prediction on CMU-MOSEI with early fusion of text, audio, and visual features</p>

    <button class="hamburger" id="menu-toggle" aria-label="Toggle menu">‚ò∞</button>

    <nav id="menu">
      <a href="../index.html">Home</a>
      <a href="../projects.html" class="dropbtn">Projects</a>
      <a href="../publications.html" class="dropbtn">Publications</a>
      <a href="../experiences.html">Experiences</a>
      <a href="../about.html">About</a>
      <a href="../contact.html">Contact</a>
      <a href="../resume.html">Resume/CV</a>
      <button id="darkToggle" aria-label="Toggle dark mode">üåô</button>
    </nav>
  </header>

  <main>
    <section class="project-section">
      <img src="../images/mosei.png" alt="CMU-MOSEI Sentiment Screenshot" class="project-img" />

      <h2>Overview</h2>
      <p align="justify">
        This project tackles multimodal sentiment analysis using the CMU-MOSEI dataset, integrating text, audio, and
        visual inputs. Each modality is encoded using a pre-trained BERT model, and their embeddings are fused early to
        jointly model sentiment. This early fusion technique enables better cross-modal interaction understanding.
      </p>
      <p align="justify">
        The model achieves a 97.87% 7-class accuracy and 0.9682 F1-score on the test set. The low MAE of 0.1060 further
        confirms its precision. Training involved dropout, Adam optimization (lr=1e-4), and early stopping. Evaluation
        metrics span 7-class and binary sentiment predictions.
      </p>

      <h2>Pipeline</h2>
      <p>
        1. Process text, audio, and visual inputs using BERT encoders<br>
        2. Concatenate modality-specific embeddings using early fusion<br>
        3. Pass through a classification head with ReLU, LayerNorm, Dropout<br>
        4. Predict 7-class sentiment using softmax<br>
        5. Train using cross-entropy loss and monitor validation metrics
      </p>

      <h2>Model & Training</h2>
      <p align="justify">
        Each modality is encoded separately using a BERT-based encoder. Audio and visual sequences are processed and
        mean pooled. The model uses early fusion with cross-attention across modalities and a dense classifier. It was
        trained for 50 epochs with early stopping at epoch 25. Key configurations include:<br>
        ‚Ä¢ Optimizer: Adam (lr = 1e-4)<br>
        ‚Ä¢ Dropout: 0.3<br>
        ‚Ä¢ Batch size: 32<br>
        ‚Ä¢ 8 transformer layers, 16 heads<br>
        ‚Ä¢ Patience = 10, seed = 42
      </p>

      <h2>Experiment Results</h2>
      <p>
        ‚Ä¢ Test 7-class Accuracy: 97.87%<br>
        ‚Ä¢ Test F1 Score: 0.9682<br>
        ‚Ä¢ Test MAE: 0.1060<br>
        ‚Ä¢ Binary Accuracy: 95.44%<br>
        ‚Ä¢ Binary F1 Score: 0.9767
      </p>

      <h2>Tech. Stack</h2>
      <p>Python, PyTorch, Transformers, BERT, CMU-MOSEI SDK, COVAREP, OpenFace, Multimodal Fusion</p>

      <h2>Links</h2>
      <p>
        <a href="https://github.com/JugalGajjar/Multimodal-Sentiment-Analysis-with-MOSEI-Dataset" target="_blank"
          class="btn">GitHub Repository</a>
        <a href="../publications/mosei.html" class="btn btn-secondary">Publication Page</a>
      </p>
    </section>

    <p class="back-home"><a href="../projects.html" class="btn btn-outline">‚Üê Back to Projects</a></p>
  </main>

  <footer>
    <p>&copy; 2025 Jugal Gajjar. All rights reserved.</p>
  </footer>
</body>

</html>